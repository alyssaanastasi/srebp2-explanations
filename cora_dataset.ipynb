{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cora Dataset Example\n",
    "\n",
    "In this example, I am given the ground truth labels of a small subset of nodes and want to infer the labels for all of the remaining nodes.\n",
    "\n",
    "The dataset used is the *Cora* dataset, which is a citation network where nodes represent documents. This dataset consists of 2708 scientific publications classified into one of seven classes. Each node is described by a 1433-dim bag of words feature vector. Two nodes (documents) are connected if there exists a citation link between them.\n",
    "\n",
    "**The task is to infer the category of each document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary packages\n",
    "\n",
    "%pip install torch_geometric # install pytorch geometric\n",
    "%pip install torchvision #install torchvision\n",
    "%pip install matplotlib #install matplotlib\n",
    "%pip install graphviz # install graphviz\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.explain import GNNExplainer,Explainer,GraphMaskExplainer,PGExplainer\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv\n",
    "from torch.nn import Linear,Softmax\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root = 'data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "# the normalize features transformation row-normalizes the attributes to sum up functional name\n",
    "\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "#get the graph object in the Cora dataset\n",
    "data = dataset[0]\n",
    "\n",
    "print()\n",
    "print(\"Information about dataset:\")\n",
    "print(data)\n",
    "print(\"Here we can see that the data is formatted as a vector of 2708 rows for 2708 publications with 1433 columns with the bag of words vector\")\n",
    "print('===========================================================================================================')\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this dataset holds 2708 nodes with 10556 edges. We are given the ground truth categories of 140 nodes and therefore, have a training node label rate of 5%.\n",
    "\n",
    "The `val_mask` and `test_mask` denote which nodes should be used for validation and testing.\n",
    "\n",
    "We want to try to infer the category of the document solely based on its content (bag-of-words feature representation) without the relational information (edges).\n",
    "\n",
    "## Training the GNN\n",
    "\n",
    "We can easily convert our MLP to a GNN by swapping the `torch.nn.Linear` layers with PyG's GNN operators.\n",
    "\n",
    "The GCN Layer is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{(\\ell + 1)}$ denotes a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge.\n",
    "In contrast, a single `Linear` layer is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\mathbf{x}_v^{(\\ell)}\n",
    "$$\n",
    "\n",
    "which does not make use of neighboring node information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "  def __init__(self, hidden_channels):\n",
    "    super().__init__()\n",
    "    torch.manual_seed(1234567)\n",
    "    self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "    self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    x = self.conv1(x, edge_index)\n",
    "    x = x.relu()\n",
    "    x = F.dropout(x, p=0.5, training=self.training)\n",
    "    x = self.conv2(x, edge_index)\n",
    "    return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  out = model(data.x, data.edge_index)\n",
    "  # this performs a single forward pass\n",
    "  loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "  # ^ Compute the loss solely based on the training nodes.\n",
    "  loss.backward() # derive gradients\n",
    "  optimizer.step() # update parameters based on gradients\n",
    "  return loss\n",
    "\n",
    "def test():\n",
    "  model.eval()\n",
    "  out = model(data.x, data.edge_index)\n",
    "  pred = out.argmax(dim=1)\n",
    "  test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "  test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "  return test_acc\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Explainer\n",
    "\n",
    "Now, I am going to try using GNN Explainer as an interpretability method on this node classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "\n",
    "\"\"\"\n",
    "GNNEXPLAINER will generate an explanation by identifying a subgraph of the computation\n",
    "graph and a subset of node features that are most influential for the model Φ’s prediction\n",
    "\n",
    "Given a node v, our goal is to identify a subgraph GS ⊆ Gc and the associated features XS =\n",
    "{xj |vj ∈ GS} that are important for the GNN’s prediction y\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes', #  mask each node feature separately across all nodes\n",
    "    edge_mask_type='object', # mask each edge\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',  # Model returns log probabilities.\n",
    "    ),\n",
    ")\n",
    "\n",
    "explanation = explainer(data.x, data.edge_index, index=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.visualize_feature_importance(top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.visualize_graph(backend='networkx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fidelity evaluates the contribution of the produced explanatory subgraph to the initial prediction,\n",
    "either by giving only the subgraph to the model (fidelity-) or by removing it from the entire graph (fidelity+).\n",
    "\n",
    "Fidelity- close to 0 is good because it shows that the the explainer is better. The explanation given is quite sufficient.\n",
    "\n",
    "The fidelity scores capture how good an explainable model reproduces the natural phenomenon or the GNN model logic.\n",
    "Once we have produced an explanation we can obtain both fidelities as:\n",
    "\"\"\"\n",
    "\n",
    "from torch_geometric.explain.metric import fidelity\n",
    "\n",
    "fid_pm = fidelity(explainer, explanation)\n",
    "print(fid_pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PG Explainer\n",
    "\n",
    "Next, we will try to use PGExplainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import PGExplainer, ModelConfig\n",
    "\n",
    "explainer2 = Explainer(\n",
    "    model=model,\n",
    "    algorithm=PGExplainer(epochs=30, lr=0.003),\n",
    "    explanation_type='phenomenon',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',  # Model returns log probabilities.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train against a variety of node-level or graph-level predictions:\n",
    "for epoch in range(30): # Indices to train against.\n",
    "        loss = explainer2.algorithm.train(epoch, model, data.x, data.edge_index,\n",
    "                                         target=data.y, index=10)\n",
    "# Get the final explanations:\n",
    "explanation = explainer2(data.x, data.edge_index, target=data.y, index=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.visualize_graph(backend='networkx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_pm = fidelity(explainer, explanation)\n",
    "print(fid_pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphMask\n",
    "\n",
    "Now, we will try to use GraphMask Explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.explain import GraphMaskExplainer\n",
    "\n",
    "explainer3 = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GraphMaskExplainer(2, epochs=5),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n",
    "\n",
    "node_index = 10\n",
    "explanation = explainer3(data.x, data.edge_index, index=node_index)\n",
    "print(f'Generated explanations in {explanation.available_explanations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.visualize_feature_importance(top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.visualize_graph(backend='networkx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_graphmask = fidelity(explainer3, explanation)\n",
    "print(fid_graphmask)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
